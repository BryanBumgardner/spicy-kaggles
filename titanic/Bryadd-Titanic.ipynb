{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n",
    "\n",
    "# this is messy and still-broken code from the link above. \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy as sp\n",
    "import sklearn \n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Model Algoriths\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from xgboost import XGBClassifier # conda install -c conda-forge xgboost\n",
    "\n",
    "#Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# Ignore the shitload of conversion warnings we get. \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Configure Visualization Defaults\n",
    "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8\n",
    "\n",
    "# Files live in the same folder as this notebook. \n",
    "submission_example = pd.read_csv('gender_submission.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Making a copy to play with. \n",
    "data1 = train.copy(deep = True)\n",
    "\n",
    "# Making a list of both trains so we can clean them at once later.\n",
    "data_cleaner = [data1, train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're familiar with the data we need to first clean it.\n",
    "\n",
    "The 4 C's:\n",
    "Correcting - remove broken data. Like if age is 800 somewhere or something. Doesn't look like it.\n",
    "\n",
    "Completing - filling null values. Many algorithms don't know how to deal so we need to fix. We need to impute missing values especially for age. We might need to change this process if we realize that filling it with the mean or something isn't working well. What I'm reading suggests we should use the median for age, drop the 'cabin' column and use mode to impute 'embark'. \n",
    "\n",
    "Create - Feature engineering\n",
    "\n",
    "Converting - changing over dates or data types that don't work well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.info()\n",
    "# train.describe()\n",
    "# train.sample(10)\n",
    "\n",
    "# going to work on having prettier print functions in this notebook.\n",
    "\n",
    "# print('Train columns with null values:\\n', data1.isnull().sum())\n",
    "# print(\"-\"*10)\n",
    "\n",
    "# print('Test columns with nulls:\\n', test.isnull().sum())\n",
    "# print(\"-\"*10)\n",
    "\n",
    "# looks like the ratio of missing age and cabin are the same across the train and test sets.\n",
    "# proof the sample is actually random between the two. \n",
    "# We need to fix these two columns along with Embarked if we can hope to model correctly.\n",
    "# in the future I would make several different versions of these dataframes,\n",
    "# testing different imputation methods to see which one works the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr        517\n",
      "Miss      182\n",
      "Mrs       125\n",
      "Master     40\n",
      "Misc       27\n",
      "Name: Title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filling the data\n",
    "\n",
    "for dataset in data_cleaner: # do em both at once\n",
    "    # Fill missing age\n",
    "    dataset['Age'].fillna(dataset['Age'].median(), inplace = True) # this doesn't work well without inplace\n",
    "    \n",
    "    # fill embarked\n",
    "    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n",
    "    \n",
    "    # fill missing fare with median\n",
    "    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n",
    "    \n",
    "    # we need to drop Passenger ID and ticket because they're just random identifiers with no purpose\n",
    "    # we also want to drop Cabin because it has too many Nulls\n",
    "\n",
    "drop_column = ['PassengerId', 'Cabin', 'Ticket'] # make a list it's easier\n",
    "data1.drop(drop_column, axis=1, inplace = True) # axis means column, inplace makes it persistent without needing to make a new variable\n",
    "\n",
    "\n",
    "\n",
    "# Time to create some features for both datasets. \n",
    "\n",
    "for dataset in data_cleaner:\n",
    "    # How about creating family size per person? \n",
    "    # makes sense that families would work together to survive\n",
    "    # and families prioritized in lifeboats\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1 # plus one to account for the person themselves\n",
    "    \n",
    "    dataset['IsAlone'] = 1\n",
    "    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0\n",
    "    # if you are alone, it's a 1, if not, it's a zero\n",
    "    # this is a binary column\n",
    "    \n",
    "    # The names have titles with them, \"Mr\" \"Miss\" \"Master\" so let's cut those off and turn them into a feature!\n",
    "    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "    \n",
    "    # We should also place the fares into bins. I don't really know how this works but I'm going to try qcut\n",
    "    # Ref: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html\n",
    "    # Ref: https://stackoverflow.com/questions/30211923/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n",
    "    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n",
    "    \n",
    "    # Using regular cut to bin the ages:\n",
    "    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n",
    "    \n",
    "    \n",
    "\n",
    "# So here's where I'm getting some guidance from other notebooks. We need to clean up rare title names.\n",
    "# print(data1['Title'].value_counts())\n",
    "# Like, lmaowtf: https://en.wikipedia.org/wiki/Jonkheer\n",
    "\n",
    "stat_min = 10 \n",
    "# Using ten as the minimum because this article says so: http://nicholasjjackson.com/2012/03/08/sample-size-is-10-a-magic-number/\n",
    "title_names = (data1['Title'].value_counts() < stat_min) # creates a true/false series with title name as the index\n",
    "\n",
    "# What we're going to do is replace the random ones below ten. \n",
    "# lambda functions \n",
    "\n",
    "data1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n",
    "print(data1['Title'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert categorical data to dummy variables for mathematical analysis. We're going to encode using the inherent sklearn and pandas tools, nothing fancy. \n",
    "\n",
    "I know that SciKit has a new library called ColumnTransformer that has replaced LabelEncoding but I haven't learned how to use it yet. \n",
    "\n",
    "https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621\n",
    "\n",
    "Let's try to use this next time:\n",
    "https://towardsdatascience.com/columntransformer-in-scikit-for-labelencoding-and-onehotencoding-in-machine-learning-c6255952731b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in data_cleaner:\n",
    "    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n",
    "    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n",
    "    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n",
    "    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n",
    "    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n",
    "\n",
    "# We now can define a y variable, the target outcome:\n",
    "Target = ['Survived']\n",
    "\n",
    "# Defining the X variables for feature selection\n",
    "data1_x = ['Sex', 'Pclass', 'Embarked', 'Title', 'SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] # pretty names for charts\n",
    "data1_x_calc = ['Sex_Code', 'Pclass', 'Embarked_Code', 'Title_Code', 'SibSp', 'Parch', 'Age', 'Fare'] # These are the actual coded columns we're gonna use\n",
    "\n",
    "data1_xy = Target + data1_x # combining them \n",
    "\n",
    "# Define the x variables for the origiunal data with bin features to remove any continuous variables\n",
    "data1_x_bin = ['Sex_Code', 'Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\n",
    "data_xy_bin = Target + data1_x_bin  # so what we've done is made two versions of the test data, one continuous and one binned\n",
    "\n",
    "# We need to turn the categorical variables into dummy variables, putting them in their own columns with binary 1/0 data inside them.\n",
    "data1_dummy = pd.get_dummies(data1[data1_x]) # ran this on that list of column names from above\n",
    "data1_x_dummy = data1_dummy.columns.tolist()\n",
    "data1_xy_dummy = Target + data1_x_dummy # add the target variable, naturally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Title_Master</th>\n",
       "      <th>Title_Misc</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  SibSp  Parch   Age     Fare  FamilySize  IsAlone  Sex_female  \\\n",
       "0       3      1      0  22.0   7.2500           2        0           0   \n",
       "1       1      1      0  38.0  71.2833           2        0           1   \n",
       "2       3      0      0  26.0   7.9250           1        1           1   \n",
       "3       1      1      0  35.0  53.1000           2        0           1   \n",
       "4       3      0      0  35.0   8.0500           1        1           0   \n",
       "\n",
       "   Sex_male  Embarked_C  Embarked_Q  Embarked_S  Title_Master  Title_Misc  \\\n",
       "0         1           0           0           1             0           0   \n",
       "1         0           1           0           0             0           0   \n",
       "2         0           0           0           1             0           0   \n",
       "3         0           0           0           1             0           0   \n",
       "4         1           0           0           1             0           0   \n",
       "\n",
       "   Title_Miss  Title_Mr  Title_Mrs  \n",
       "0           0         1          0  \n",
       "1           0         0          1  \n",
       "2           1         0          0  \n",
       "3           0         0          1  \n",
       "4           0         1          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1_dummy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've put together all our data, we need to split some test and train data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first split using randomstate\n",
    "train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0) # using the preset variables from above to keep this function clean. \n",
    "train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target], random_state = 0) # doing the same, but with bins\n",
    "\n",
    "train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is clean, we're going to explore the data and see if we can find some basic correlations to inform what we do next. I have some ideas, but for the sake of science let's examine everything equally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival Correlation by: Sex\n",
      "      Sex  Survived\n",
      "0  female  0.742038\n",
      "1    male  0.188908\n",
      "Survival Correlation by: Pclass\n",
      "   Pclass  Survived\n",
      "0       1  0.629630\n",
      "1       2  0.472826\n",
      "2       3  0.242363\n",
      "Survival Correlation by: Embarked\n",
      "  Embarked  Survived\n",
      "0        C  0.553571\n",
      "1        Q  0.389610\n",
      "2        S  0.339009\n",
      "Survival Correlation by: Title\n",
      "    Title  Survived\n",
      "0  Master  0.575000\n",
      "1    Misc  0.444444\n",
      "2    Miss  0.697802\n",
      "3      Mr  0.156673\n",
      "4     Mrs  0.792000\n",
      "Survival Correlation by: SibSp\n",
      "   SibSp  Survived\n",
      "0      0  0.345395\n",
      "1      1  0.535885\n",
      "2      2  0.464286\n",
      "3      3  0.250000\n",
      "4      4  0.166667\n",
      "5      5  0.000000\n",
      "6      8  0.000000\n",
      "Survival Correlation by: Parch\n",
      "   Parch  Survived\n",
      "0      0  0.343658\n",
      "1      1  0.550847\n",
      "2      2  0.500000\n",
      "3      3  0.600000\n",
      "4      4  0.000000\n",
      "5      5  0.200000\n",
      "6      6  0.000000\n",
      "Survival Correlation by: FamilySize\n",
      "   FamilySize  Survived\n",
      "0           1  0.303538\n",
      "1           2  0.552795\n",
      "2           3  0.578431\n",
      "3           4  0.724138\n",
      "4           5  0.200000\n",
      "5           6  0.136364\n",
      "6           7  0.333333\n",
      "7           8  0.000000\n",
      "8          11  0.000000\n",
      "Survival Correlation by: IsAlone\n",
      "   IsAlone  Survived\n",
      "0        0  0.505650\n",
      "1        1  0.303538\n",
      "Survived    0    1\n",
      "Title             \n",
      "Master     17   23\n",
      "Misc       15   12\n",
      "Miss       55  127\n",
      "Mr        436   81\n",
      "Mrs        26   99\n"
     ]
    }
   ],
   "source": [
    "for x in data1_x:\n",
    "     if data1[x].dtype != 'float64' : \n",
    "            print('Survival Correlation by:', x)\n",
    "            print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n",
    "\n",
    "print(pd.crosstab(data1['Title'], data1[Target[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipping visualization because we need faster, better ways to do it. Next step, modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"No Free Lunch Theorem\" = there is no super algorithm that works best. The best approach is to try several and pick the one that works best for your situation.\n",
    "# to that end, we're going to set up a bunch and try them.\n",
    "\n",
    "# I love this format because this is some copy/paste code we can run again to quickly run the data through every single algorithm, score them and easily visualize. \n",
    "# Should we turn this into a function eventually? I'm game for it \n",
    "\n",
    "MLA = [\n",
    "    # Ensemble\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "    \n",
    "    # Gaussian Process\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    # GLM\n",
    "    linear_model.LogisticRegression(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.RidgeClassifier(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    \n",
    "    # Naive Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    # Nearest Neighbors\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "    # SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    \n",
    "    # Trees\n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    \n",
    "    # Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "    \n",
    "    #xgboost MONEY GANG\n",
    "    XGBClassifier()\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to try splitting in cross-validation with this splitter class, which is an alternative to train_test_split\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0) # this intentionally leaves out 10% of the model - I am not sure why this is a good idea but I hear it is\n",
    "\n",
    "# We are going to create a table to compare all the MLAs and see what their score is\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD', 'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "# Create table to compare MLA predictions\n",
    "MLA_predict = data1[Target]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index through the MLA and save the performance to put it in a table - this is the money moment. \n",
    "# Really should wrap this in a function for ease of use later. Does one of those exist already, maybe?\n",
    "\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "    \n",
    "    # set the name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    # We are going to score the model with cross validation.\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    \n",
    "    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv = cv_split, return_train_score=True) # The code has changed since the guide went live. You now have to tell it to return the train score. \n",
    "    \n",
    "    # simply reference the cv_results output column by column and format it the way we want. \n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean() # How long does it take? We're speed racers out here\n",
    "\n",
    "\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()\n",
    "    \n",
    "    # If this is a non-bias random sample, then +/- 3 standard deviations from the mean should statistically capture 99.7% of the subsets. \n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3 # This will tell us the worst that can happen\n",
    "    \n",
    "    # save them for export\n",
    "    alg.fit(data1[data1_x_bin], data1[Target])\n",
    "    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n",
    "    \n",
    "    row_index+=1 # add one to the row index so it moves to the next row.\n",
    "    \n",
    "    \n",
    "# print and sort the table. \n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending=False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perry aye "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLA Name</th>\n",
       "      <th>MLA Parameters</th>\n",
       "      <th>MLA Train Accuracy Mean</th>\n",
       "      <th>MLA Test Accuracy Mean</th>\n",
       "      <th>MLA Test Accuracy 3*STD</th>\n",
       "      <th>MLA Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>{'base_score': 0.5, 'booster': 'gbtree', 'cols...</td>\n",
       "      <td>0.856367</td>\n",
       "      <td>0.829478</td>\n",
       "      <td>0.0527546</td>\n",
       "      <td>0.0416277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SVC</td>\n",
       "      <td>{'C': 1.0, 'cache_size': 200, 'class_weight': ...</td>\n",
       "      <td>0.837266</td>\n",
       "      <td>0.826119</td>\n",
       "      <td>0.0453876</td>\n",
       "      <td>0.0421442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>{'criterion': 'friedman_mse', 'init': None, 'l...</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.822761</td>\n",
       "      <td>0.0498731</td>\n",
       "      <td>0.0647264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'bootstrap': True, 'class_weight': None, 'cri...</td>\n",
       "      <td>0.891199</td>\n",
       "      <td>0.822761</td>\n",
       "      <td>0.0488578</td>\n",
       "      <td>0.0122359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NuSVC</td>\n",
       "      <td>{'cache_size': 200, 'class_weight': None, 'coe...</td>\n",
       "      <td>0.835768</td>\n",
       "      <td>0.822761</td>\n",
       "      <td>0.0493681</td>\n",
       "      <td>0.0315578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      MLA Name  \\\n",
       "21               XGBClassifier   \n",
       "14                         SVC   \n",
       "3   GradientBoostingClassifier   \n",
       "4       RandomForestClassifier   \n",
       "15                       NuSVC   \n",
       "\n",
       "                                       MLA Parameters MLA Train Accuracy Mean  \\\n",
       "21  {'base_score': 0.5, 'booster': 'gbtree', 'cols...                0.856367   \n",
       "14  {'C': 1.0, 'cache_size': 200, 'class_weight': ...                0.837266   \n",
       "3   {'criterion': 'friedman_mse', 'init': None, 'l...                0.866667   \n",
       "4   {'bootstrap': True, 'class_weight': None, 'cri...                0.891199   \n",
       "15  {'cache_size': 200, 'class_weight': None, 'coe...                0.835768   \n",
       "\n",
       "   MLA Test Accuracy Mean MLA Test Accuracy 3*STD   MLA Time  \n",
       "21               0.829478               0.0527546  0.0416277  \n",
       "14               0.826119               0.0453876  0.0421442  \n",
       "3                0.822761               0.0498731  0.0647264  \n",
       "4                0.822761               0.0488578  0.0122359  \n",
       "15               0.822761               0.0493681  0.0315578  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLA_compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Algorithm')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# lets take a look eh\n",
    "sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n",
    "\n",
    "#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "plt.title('Machine Learning Algorithm Accuracy Score \\n')\n",
    "plt.xlabel('Accuracy Score (%)')\n",
    "plt.ylabel('Algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So right out the gate we were able to get like 82% which is phenomenal. The next step is to figure out - is it going to be worth our time to improve this? If we are going to get a five percent increase for 3 months of work, maybe. But if we get .0001% increase no way. \n",
    "\n",
    "Before we decide how to make our model better we need to decide if it's worth keeping. One trick is to look at the data ourselves. We know that 67.5% of the people on the titanic died. If we guess on each person that each person died, we'd be right 67.5% of the time. So that's our baseline - anything lower than 68% is bad model performance, because why would I need it? We could just say everyone died and get a better score. \n",
    "\n",
    "Before we do that, let's just look at a manual decision tree, something we can do every time we look at data to see if just screwing around we can get a better model than a computer. This defines our \"baseline\" of performance to know if we should try to make it better or not. \n",
    "\n",
    "\n",
    "Copy and pasting the below for reference from this guy's guide. \n",
    "\n",
    "Our rule of thumb will be the majority rules. Meaning, if the majority or 50% or more survived, then everybody in our subgroup survived/1, but if 50% or less survived then if everybody in our subgroup died/0. Also, we will stop if the subgroup is less than 10 and/or our model accuracy plateaus or decreases. Got it? Let's go!\n",
    "\n",
    "Question 1: Were you on the Titanic? If Yes, then majority (62%) died. Note our sample survival is different than our population of 68%. Nonetheless, if we assumed everybody died, our sample accuracy is 62%.\n",
    "\n",
    "Question 2: Are you male or female? Male, majority (81%) died. Female, majority (74%) survived. Giving us an accuracy of 79%.\n",
    "\n",
    "Question 3A (going down the female branch with count = 314): Are you in class 1, 2, or 3? Class 1, majority (97%) survived and Class 2, majority (92%) survived. Since the dead subgroup is less than 10, we will stop going down this branch. Class 3, is even at a 50-50 split. No new information to improve our model is gained.\n",
    "\n",
    "Question 4A (going down the female class 3 branch with count = 144): Did you embark from port C, Q, or S? We gain a little information. C and Q, the majority still survived, so no change. Also, the dead subgroup is less than 10, so we will stop. S, the majority (63%) died. So, we will change females, class 3, embarked S from assuming they survived, to assuming they died. Our model accuracy increases to 81%.\n",
    "\n",
    "Question 5A (going down the female class 3 embarked S branch with count = 88): So far, it looks like we made good decisions. Adding another level does not seem to gain much more information. This subgroup 55 died and 33 survived, since majority died we need to find a signal to identify the 33 or a subgroup to change them from dead to survived and improve our model accuracy. We can play with our features. One I found was fare 0-8, majority survived. It's a small sample size 11-9, but one often used in statistics. We slightly improve our accuracy, but not much to move us past 82%. So, we'll stop here.\n",
    "\n",
    "Question 3B (going down the male branch with count = 577): Going back to question 2, we know the majority of males died. So, we are looking for a feature that identifies a subgroup that majority survived. Surprisingly, class or even embarked didn't matter like it did for females, but title does and gets us to 82%. Guess and checking other features, none seem to push us past 82%. So, we'll stop here for now.\n",
    "\n",
    "You did it, with very little information, we get to 82% accuracy. On a worst, bad, good, better, and best scale, we'll set 82% to good, since it's a simple model that yields us decent results. But the question still remains, can we do better than our handmade model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE DT Parameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': False, 'random_state': 0, 'splitter': 'best'}\n",
      "BEFORE DT Training w/bin score mean: 89.51\n",
      "BEFORE DT Test w/bin score mean: 82.09\n",
      "BEFORE DT Test w/bin score 3*std: +/- 5.57\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# time to tune hyperparameters\n",
    "# This is the base model. \n",
    "\n",
    "dtree = tree.DecisionTreeClassifier(random_state = 0)\n",
    "base_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv = cv_split, return_train_score=True)\n",
    "dtree.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print('BEFORE DT Parameters: ', dtree.get_params())\n",
    "print(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "#print(\"BEFORE DT Test w/bin set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get some extensions in here like snippets to help you do this faster, don't forget! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER DT Parameters:  {'criterion': 'gini', 'max_depth': 4, 'random_state': 0}\n",
      "AFTER DT Training w/bin score mean: 89.35\n",
      "AFTER DT Test w/bin score mean: 87.40\n",
      "AFTER DT Test w/bin score 3*std: +/- 5.00\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Tune the hyper-parameters: \n",
    "param_grid = {'criterion': ['gini', 'entropy'], #scoring methodology; two supported formulas for calculating information gain - default is gini\n",
    "             'max_depth': [2,4,6,8,10,None], # max depth tree can grow, default is none\n",
    "              #'min_samples_split': [2,5,10,.03,.05], #minimum subset size BEFORE new split (fraction is % of total); default is 2\n",
    "              #'min_samples_leaf': [1,5,10,.03,.05], #minimum subset size AFTER new split split (fraction is % of total); default is 1\n",
    "              #'max_features': [None, 'auto'], #max features to consider when performing split; default none or all\n",
    "              'random_state' : [0] # seed or control random number generator\n",
    "             }\n",
    "\n",
    "#print(list(model_selection.ParameterGrid(param_grid)))\n",
    "\n",
    "#choose best model with grid_search: #http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n",
    "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "\n",
    "tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True) \n",
    "# remember, the return_train_score thing is a new bit that isn't covered in the now-outdated guide we're following. \n",
    "tune_model.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "#print(tune_model.cv_results_.keys())\n",
    "#print(tune_model.cv_results_['params'])\n",
    "print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "#print(tune_model.cv_results_['mean_train_score'])\n",
    "print(\"AFTER DT Training w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(tune_model.cv_results_['mean_test_score'])\n",
    "print(\"AFTER DT Test w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT Test w/bin score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to tune with feature selection. As stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. Sklearn has several options, we will use recursive feature elimination (RFE) with cross validation (CV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE DT RFE Training Shape Old:  (891, 7)\n",
      "BEFORE DT RFE Training Columns Old:  ['Sex_Code' 'Pclass' 'Embarked_Code' 'Title_Code' 'FamilySize'\n",
      " 'AgeBin_Code' 'FareBin_Code']\n",
      "BEFORE DT RFE Training w/bin score mean: 89.51\n",
      "BEFORE DT RFE Test w/bin score mean: 82.09\n",
      "BEFORE DT RFE Test w/bin score 3*std: +/- 5.57\n",
      "----------\n",
      "AFTER DT RFE Training Shape New:  (891, 6)\n",
      "AFTER DT RFE Training Columns New:  ['Sex_Code' 'Pclass' 'Title_Code' 'FamilySize' 'AgeBin_Code'\n",
      " 'FareBin_Code']\n",
      "AFTER DT RFE Training w/bin score mean: 88.16\n",
      "AFTER DT RFE Test w/bin score mean: 83.06\n",
      "AFTER DT RFE Test w/bin score 3*std: +/- 6.22\n",
      "----------\n",
      "AFTER DT RFE Tuned Parameters:  {'criterion': 'gini', 'max_depth': 4, 'random_state': 0}\n",
      "AFTER DT RFE Tuned Training w/bin score mean: 89.39\n",
      "AFTER DT RFE Tuned Test w/bin score mean: 87.34\n",
      "AFTER DT RFE Tuned Test w/bin score 3*std: +/- 6.21\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# the base model:\n",
    "print('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \n",
    "print('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n",
    "\n",
    "print(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "# feature selection\n",
    "dtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)\n",
    "dtree_rfe.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "#transform x&y to reduced features and fit new model\n",
    "#alternative: can use pipeline to reduce fit and transform steps: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "X_rfe = data1[data1_x_bin].columns.values[dtree_rfe.get_support()]\n",
    "rfe_results = model_selection.cross_validate(dtree, data1[X_rfe], data1[Target], cv=cv_split, return_train_score=True)\n",
    "\n",
    "#print(dtree_rfe.grid_scores_)\n",
    "print('AFTER DT RFE Training Shape New: ', data1[X_rfe].shape) \n",
    "print('AFTER DT RFE Training Columns New: ', X_rfe)\n",
    "\n",
    "print(\"AFTER DT RFE Training w/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \n",
    "print(\"AFTER DT RFE Test w/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\n",
    "print(\"AFTER DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "#tune rfe model\n",
    "rfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True)\n",
    "rfe_tune_model.fit(data1[X_rfe], data1[Target])\n",
    "\n",
    "#print(rfe_tune_model.cv_results_.keys())\n",
    "#print(rfe_tune_model.cv_results_['params'])\n",
    "print('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\n",
    "#print(rfe_tune_model.cv_results_['mean_train_score'])\n",
    "print(\"AFTER DT RFE Tuned Training w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(rfe_tune_model.cv_results_['mean_test_score'])\n",
    "print(\"AFTER DT RFE Tuned Test w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT RFE Tuned Test w/bin score 3*std: +/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-4d047082f34f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m dot_data = tree.export_graphviz(dtree, out_file=None, \n\u001b[1;32m      5\u001b[0m                                 \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata1_x_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
    "import graphviz \n",
    "from sklearn import tree\n",
    "dot_data = tree.export_graphviz(dtree, out_file=None, \n",
    "                                feature_names = data1_x_bin, class_names = True,\n",
    "                                filled = True, rounded = True)\n",
    "graph = graphviz.Source(dot_data) \n",
    "graph\n",
    "\n",
    "# since this doesn't work, we need some way to visualize the decision tree here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'correlation_heatmap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-5724015b9e91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcorrelation_heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLA_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'correlation_heatmap' is not defined"
     ]
    }
   ],
   "source": [
    "#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n",
    "#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\n",
    "correlation_heatmap(MLA_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting Training w/bin score mean: 86.54\n",
      "Hard Voting Test w/bin score mean: 82.28\n",
      "Hard Voting Test w/bin score 3*std: +/- 4.70\n",
      "----------\n",
      "Soft Voting Training w/bin score mean: 87.30\n",
      "Soft Voting Test w/bin score mean: 82.28\n",
      "Soft Voting Test w/bin score 3*std: +/- 4.26\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "#why choose one model, when you can pick them all with voting classifier\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\n",
    "vote_est = [\n",
    "    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n",
    "    ('ada', ensemble.AdaBoostClassifier()),\n",
    "    ('bc', ensemble.BaggingClassifier()),\n",
    "    ('etc',ensemble.ExtraTreesClassifier()),\n",
    "    ('gbc', ensemble.GradientBoostingClassifier()),\n",
    "    ('rfc', ensemble.RandomForestClassifier()),\n",
    "\n",
    "    #Gaussian Processes: http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc\n",
    "    ('gpc', gaussian_process.GaussianProcessClassifier()),\n",
    "    \n",
    "    #GLM: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "    ('lr', linear_model.LogisticRegressionCV()),\n",
    "    \n",
    "    #Navies Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "    ('bnb', naive_bayes.BernoulliNB()),\n",
    "    ('gnb', naive_bayes.GaussianNB()),\n",
    "    \n",
    "    #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html\n",
    "    ('knn', neighbors.KNeighborsClassifier()),\n",
    "    \n",
    "    #SVM: http://scikit-learn.org/stable/modules/svm.html\n",
    "    ('svc', svm.SVC(probability=True)),\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "   ('xgb', XGBClassifier())\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "#Hard Vote or majority rules\n",
    "vote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
    "vote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True)\n",
    "vote_hard.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Hard Voting Training w/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \n",
    "print(\"Hard Voting Test w/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\n",
    "print(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#Soft Vote or weighted probabilities\n",
    "vote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
    "vote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True)\n",
    "vote_soft.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Soft Voting Training w/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \n",
    "print(\"Soft Voting Test w/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\n",
    "print(\"Soft Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameter for AdaBoostClassifier is {'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0} with a runtime of 27.63 seconds.\n",
      "The best parameter for BaggingClassifier is {'max_samples': 0.25, 'n_estimators': 300, 'random_state': 0} with a runtime of 22.82 seconds.\n",
      "The best parameter for ExtraTreesClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0} with a runtime of 46.98 seconds.\n",
      "The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 35.31 seconds.\n",
      "The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 49.58 seconds.\n",
      "The best parameter for GaussianProcessClassifier is {'max_iter_predict': 10, 'random_state': 0} with a runtime of 4.50 seconds.\n",
      "The best parameter for LogisticRegressionCV is {'fit_intercept': True, 'random_state': 0, 'solver': 'liblinear'} with a runtime of 6.39 seconds.\n",
      "The best parameter for BernoulliNB is {'alpha': 0.1} with a runtime of 0.24 seconds.\n",
      "The best parameter for GaussianNB is {} with a runtime of 0.05 seconds.\n",
      "The best parameter for KNeighborsClassifier is {'algorithm': 'brute', 'n_neighbors': 7, 'weights': 'uniform'} with a runtime of 3.58 seconds.\n",
      "The best parameter for SVC is {'C': 2, 'decision_function_shape': 'ovo', 'gamma': 0.1, 'probability': True, 'random_state': 0} with a runtime of 16.62 seconds.\n",
      "The best parameter for XGBClassifier is {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0} with a runtime of 73.02 seconds.\n",
      "Total optimization time was 4.78 minutes.\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "#WARNING: Running is very computational intensive and time expensive.\n",
    "#Code is written for experimental/developmental purposes and not production ready!\n",
    "\n",
    "\n",
    "#Hyperparameter Tune with GridSearchCV: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "grid_n_estimator = [10, 50, 100, 300]\n",
    "grid_ratio = [.1, .25, .5, .75, 1.0]\n",
    "grid_learn = [.01, .03, .05, .1, .25]\n",
    "grid_max_depth = [2, 4, 6, 8, 10, None]\n",
    "grid_min_samples = [5, 10, .03, .05, .10]\n",
    "grid_criterion = ['gini', 'entropy']\n",
    "grid_bool = [True, False]\n",
    "grid_seed = [0]\n",
    "\n",
    "\n",
    "grid_param = [\n",
    "            [{\n",
    "            #AdaBoostClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "            'n_estimators': grid_n_estimator, #default=50\n",
    "            'learning_rate': grid_learn, #default=1\n",
    "            #'algorithm': ['SAMME', 'SAMME.R'], #default=SAMME.R\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "       \n",
    "    \n",
    "            [{\n",
    "            #BaggingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'max_samples': grid_ratio, #default=1.0\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "    \n",
    "            [{\n",
    "            #ExtraTreesClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'criterion': grid_criterion, #default=gini\n",
    "            'max_depth': grid_max_depth, #default=None\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "\n",
    "            [{\n",
    "            #GradientBoostingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
    "            #'loss': ['deviance', 'exponential'], #default=deviance\n",
    "            'learning_rate': [.05], #default=0.1 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n",
    "            'n_estimators': [300], #default=100 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n",
    "            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=friedman_mse\n",
    "            'max_depth': grid_max_depth, #default=3   \n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "    \n",
    "            [{\n",
    "            #RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'criterion': grid_criterion, #default=gini\n",
    "            'max_depth': grid_max_depth, #default=None\n",
    "            'oob_score': [True], #default=False -- 12/31/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{    \n",
    "            #GaussianProcessClassifier\n",
    "            'max_iter_predict': grid_n_estimator, #default: 100\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "        \n",
    "    \n",
    "            [{\n",
    "            #LogisticRegressionCV - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n",
    "            'fit_intercept': grid_bool, #default: True\n",
    "            #'penalty': ['l1','l2'],\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "            \n",
    "    \n",
    "            [{\n",
    "            #BernoulliNB - http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n",
    "            'alpha': grid_ratio, #default: 1.0\n",
    "             }],\n",
    "    \n",
    "    \n",
    "            #GaussianNB - \n",
    "            [{}],\n",
    "    \n",
    "            [{\n",
    "            #KNeighborsClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n",
    "            'weights': ['uniform', 'distance'], #default = uniform\n",
    "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "            }],\n",
    "            \n",
    "    \n",
    "            [{\n",
    "            #SVC - http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "            #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r\n",
    "            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'C': [1,2,3,4,5], #default=1.0\n",
    "            'gamma': grid_ratio, #edfault: auto\n",
    "            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n",
    "            'probability': [True],\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "    \n",
    "            [{\n",
    "            #XGBClassifier - http://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "            'learning_rate': grid_learn, #default: .3\n",
    "            'max_depth': [1,2,4,6,8,10], #default 2\n",
    "            'n_estimators': grid_n_estimator, \n",
    "            'seed': grid_seed  \n",
    "             }]   \n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "start_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counter\n",
    "for clf, param in zip (vote_est, grid_param): #https://docs.python.org/3/library/functions.html#zip\n",
    "\n",
    "    #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n",
    "    #print(param)\n",
    "    \n",
    "    \n",
    "    start = time.perf_counter()        \n",
    "    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n",
    "    best_search.fit(data1[data1_x_bin], data1[Target])\n",
    "    run = time.perf_counter() - start\n",
    "\n",
    "    best_param = best_search.best_params_\n",
    "    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n",
    "    clf[1].set_params(**best_param) \n",
    "\n",
    "\n",
    "run_total = time.perf_counter() - start_total\n",
    "print('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n",
    "\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting w/Tuned Hyperparameters Training w/bin score mean: 85.22\n",
      "Hard Voting w/Tuned Hyperparameters Test w/bin score mean: 82.31\n",
      "Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- 5.26\n",
      "----------\n",
      "Soft Voting w/Tuned Hyperparameters Training w/bin score mean: 84.78\n",
      "Soft Voting w/Tuned Hyperparameters Test w/bin score mean: 82.24\n",
      "Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- 5.40\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "#Hard Vote or majority rules w/Tuned Hyperparameters\n",
    "grid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
    "grid_hard_cv = model_selection.cross_validate(grid_hard, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True)\n",
    "grid_hard.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \n",
    "print(\"Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\n",
    "print(\"Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "#Soft Vote or weighted probabilities w/Tuned Hyperparameters\n",
    "grid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
    "grid_soft_cv = model_selection.cross_validate(grid_soft, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True)\n",
    "grid_soft.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Soft Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \n",
    "print(\"Soft Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\n",
    "print(\"Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#12/31/17 tuned with data1_x_bin\n",
    "#The best parameter for AdaBoostClassifier is {'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0} with a runtime of 33.39 seconds.\n",
    "#The best parameter for BaggingClassifier is {'max_samples': 0.25, 'n_estimators': 300, 'random_state': 0} with a runtime of 30.28 seconds.\n",
    "#The best parameter for ExtraTreesClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0} with a runtime of 64.76 seconds.\n",
    "#The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 34.35 seconds.\n",
    "#The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 76.32 seconds.\n",
    "#The best parameter for GaussianProcessClassifier is {'max_iter_predict': 10, 'random_state': 0} with a runtime of 6.01 seconds.\n",
    "#The best parameter for LogisticRegressionCV is {'fit_intercept': True, 'random_state': 0, 'solver': 'liblinear'} with a runtime of 8.04 seconds.\n",
    "#The best parameter for BernoulliNB is {'alpha': 0.1} with a runtime of 0.19 seconds.\n",
    "#The best parameter for GaussianNB is {} with a runtime of 0.04 seconds.\n",
    "#The best parameter for KNeighborsClassifier is {'algorithm': 'brute', 'n_neighbors': 7, 'weights': 'uniform'} with a runtime of 4.84 seconds.\n",
    "#The best parameter for SVC is {'C': 2, 'decision_function_shape': 'ovo', 'gamma': 0.1, 'probability': True, 'random_state': 0} with a runtime of 29.39 seconds.\n",
    "#The best parameter for XGBClassifier is {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0} with a runtime of 46.23 seconds.\n",
    "#Total optimization time was 5.56 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      "PassengerId    418 non-null int64\n",
      "Pclass         418 non-null int64\n",
      "Name           418 non-null object\n",
      "Sex            418 non-null object\n",
      "Age            332 non-null float64\n",
      "SibSp          418 non-null int64\n",
      "Parch          418 non-null int64\n",
      "Ticket         418 non-null object\n",
      "Fare           417 non-null float64\n",
      "Cabin          91 non-null object\n",
      "Embarked       418 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n",
      "None\n",
      "----------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mytree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-5e193443b482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#handmade decision tree - submission score = 0.77990\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mytree' is not defined"
     ]
    }
   ],
   "source": [
    "#prepare data for modeling\n",
    "print(test.info())\n",
    "print(\"-\"*10)\n",
    "#data_val.sample(10)\n",
    "\n",
    "\n",
    "\n",
    "#handmade decision tree - submission score = 0.77990\n",
    "test['Survived'] = mytree(test).astype(int)\n",
    "\n",
    "\n",
    "#decision tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n",
    "#submit_dt = tree.DecisionTreeClassifier()\n",
    "#submit_dt = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_dt.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_dt.best_params_) #Best Parameters:  {'criterion': 'gini', 'max_depth': 4, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_dt.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\n",
    "#submit_bc = ensemble.BaggingClassifier()\n",
    "#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_bc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n",
    "#submit_etc = ensemble.ExtraTreesClassifier()\n",
    "#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_etc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\n",
    "#submit_rfc = ensemble.RandomForestClassifier()\n",
    "#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_rfc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "\n",
    "#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\n",
    "#submit_abc = ensemble.AdaBoostClassifier()\n",
    "#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_abc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\n",
    "#submit_gbc = ensemble.GradientBoostingClassifier()\n",
    "#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_gbc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\n",
    "\n",
    "#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\n",
    "#submit_xgb = XGBClassifier()\n",
    "#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_xgb.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\n",
    "#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\n",
    "#data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\n",
    "test['Survived'] = grid_hard.predict(test[data1_x_bin])\n",
    "\n",
    "\n",
    "#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\n",
    "#data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\n",
    "#data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#submit file\n",
    "# submit = data_val[['PassengerId','Survived']]\n",
    "# submit.to_csv(\"../working/submit.csv\", index=False)\n",
    "\n",
    "print('Validation Data Distribution: \\n', test['Survived'].value_counts(normalize = True))\n",
    "# submit.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
