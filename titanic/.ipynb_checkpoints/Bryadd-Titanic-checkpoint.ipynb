{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy as sp\n",
    "import sklearn \n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Model Algoriths\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from xgboost import XGBClassifier # conda install -c conda-forge xgboost\n",
    "\n",
    "#Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#Configure Visualization Defaults\n",
    "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files live in the same folder as this notebook. \n",
    "submission_example = pd.read_csv('gender_submission.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a copy to play with. \n",
    "data1 = train.copy(deep = True)\n",
    "\n",
    "# Making a list of both trains so we can clean them at once later.\n",
    "data_cleaner = [data1, train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.info()\n",
    "# train.describe()\n",
    "# train.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're familiar with the data we need to first clean it.\n",
    "\n",
    "The 4 C's:\n",
    "Correcting - remove broken data. Like if age is 800 somewhere or something. Doesn't look like it.\n",
    "\n",
    "Completing - filling null values. Many algorithms don't know how to deal so we need to fix. We need to impute missing values especially for age. We might need to change this process if we realize that filling it with the mean or something isn't working well. What I'm reading suggests we should use the median for age, drop the 'cabin' column and use mode to impute 'embark'. \n",
    "\n",
    "Create - Feature engineering\n",
    "\n",
    "Converting - changing over dates or data types that don't work well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns with null values:\n",
      " PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "----------\n",
      "Test columns with nulls:\n",
      " PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# going to work on having prettier print functions in this notebook.\n",
    "\n",
    "print('Train columns with null values:\\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test columns with nulls:\\n', test.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "# looks like the ratio of missing age and cabin are the same across the train and test sets.\n",
    "# proof the sample is actually random between the two. \n",
    "# We need to fix these two columns along with Embarked if we can hope to model correctly.\n",
    "# in the future I would make several different versions of these dataframes,\n",
    "# testing different imputation methods to see which one works the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the data\n",
    "\n",
    "for dataset in data_cleaner: # do em both at once\n",
    "    # Fill missing age\n",
    "    dataset['Age'].fillna(dataset['Age'].median(), inplace = True) # this doesn't work well without inplace\n",
    "    \n",
    "    # fill embarked\n",
    "    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n",
    "    \n",
    "    # fill missing fare with median\n",
    "    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n",
    "    \n",
    "    # we need to drop Passenger ID and ticket because they're just random identifiers with no purpose\n",
    "    # we also want to drop Cabin because it has too many Nulls\n",
    "\n",
    "drop_column = ['PassengerId', 'Cabin', 'Ticket'] # make a list it's easier\n",
    "data1.drop(drop_column, axis=1, inplace = True) # axis means column, inplace makes it persistent without needing to make a new variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to create some features for both datasets. \n",
    "\n",
    "for dataset in data_cleaner:\n",
    "    # How about creating family size per person? \n",
    "    # makes sense that families would work together to survive\n",
    "    # and families prioritized in lifeboats\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1 # plus one to account for the person themselves\n",
    "    \n",
    "    dataset['IsAlone'] = 1\n",
    "    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0\n",
    "    # if you are alone, it's a 1, if not, it's a zero\n",
    "    # this is a binary column\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
