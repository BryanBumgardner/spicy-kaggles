{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy as sp\n",
    "import sklearn \n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Model Algoriths\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from xgboost import XGBClassifier # conda install -c conda-forge xgboost\n",
    "\n",
    "#Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#Configure Visualization Defaults\n",
    "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8\n",
    "\n",
    "# Files live in the same folder as this notebook. \n",
    "submission_example = pd.read_csv('gender_submission.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Making a copy to play with. \n",
    "data1 = train.copy(deep = True)\n",
    "\n",
    "# Making a list of both trains so we can clean them at once later.\n",
    "data_cleaner = [data1, train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're familiar with the data we need to first clean it.\n",
    "\n",
    "The 4 C's:\n",
    "Correcting - remove broken data. Like if age is 800 somewhere or something. Doesn't look like it.\n",
    "\n",
    "Completing - filling null values. Many algorithms don't know how to deal so we need to fix. We need to impute missing values especially for age. We might need to change this process if we realize that filling it with the mean or something isn't working well. What I'm reading suggests we should use the median for age, drop the 'cabin' column and use mode to impute 'embark'. \n",
    "\n",
    "Create - Feature engineering\n",
    "\n",
    "Converting - changing over dates or data types that don't work well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.info()\n",
    "# train.describe()\n",
    "# train.sample(10)\n",
    "\n",
    "# going to work on having prettier print functions in this notebook.\n",
    "\n",
    "# print('Train columns with null values:\\n', data1.isnull().sum())\n",
    "# print(\"-\"*10)\n",
    "\n",
    "# print('Test columns with nulls:\\n', test.isnull().sum())\n",
    "# print(\"-\"*10)\n",
    "\n",
    "# looks like the ratio of missing age and cabin are the same across the train and test sets.\n",
    "# proof the sample is actually random between the two. \n",
    "# We need to fix these two columns along with Embarked if we can hope to model correctly.\n",
    "# in the future I would make several different versions of these dataframes,\n",
    "# testing different imputation methods to see which one works the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr        517\n",
      "Miss      182\n",
      "Mrs       125\n",
      "Master     40\n",
      "Misc       27\n",
      "Name: Title, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "//anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Filling the data\n",
    "\n",
    "for dataset in data_cleaner: # do em both at once\n",
    "    # Fill missing age\n",
    "    dataset['Age'].fillna(dataset['Age'].median(), inplace = True) # this doesn't work well without inplace\n",
    "    \n",
    "    # fill embarked\n",
    "    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n",
    "    \n",
    "    # fill missing fare with median\n",
    "    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n",
    "    \n",
    "    # we need to drop Passenger ID and ticket because they're just random identifiers with no purpose\n",
    "    # we also want to drop Cabin because it has too many Nulls\n",
    "\n",
    "drop_column = ['PassengerId', 'Cabin', 'Ticket'] # make a list it's easier\n",
    "data1.drop(drop_column, axis=1, inplace = True) # axis means column, inplace makes it persistent without needing to make a new variable\n",
    "\n",
    "\n",
    "\n",
    "# Time to create some features for both datasets. \n",
    "\n",
    "for dataset in data_cleaner:\n",
    "    # How about creating family size per person? \n",
    "    # makes sense that families would work together to survive\n",
    "    # and families prioritized in lifeboats\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1 # plus one to account for the person themselves\n",
    "    \n",
    "    dataset['IsAlone'] = 1\n",
    "    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0\n",
    "    # if you are alone, it's a 1, if not, it's a zero\n",
    "    # this is a binary column\n",
    "    \n",
    "    # The names have titles with them, \"Mr\" \"Miss\" \"Master\" so let's cut those off and turn them into a feature!\n",
    "    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "    \n",
    "    # We should also place the fares into bins. I don't really know how this works but I'm going to try qcut\n",
    "    # Ref: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html\n",
    "    # Ref: https://stackoverflow.com/questions/30211923/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n",
    "    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n",
    "    \n",
    "    # Using regular cut to bin the ages:\n",
    "    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n",
    "    \n",
    "    \n",
    "\n",
    "# So here's where I'm getting some guidance from other notebooks. We need to clean up rare title names.\n",
    "# print(data1['Title'].value_counts())\n",
    "# Like, lmaowtf: https://en.wikipedia.org/wiki/Jonkheer\n",
    "\n",
    "stat_min = 10 \n",
    "# Using ten as the minimum because this article says so: http://nicholasjjackson.com/2012/03/08/sample-size-is-10-a-magic-number/\n",
    "title_names = (data1['Title'].value_counts() < stat_min) # creates a true/false series with title name as the index\n",
    "\n",
    "# What we're going to do is replace the random ones below ten. \n",
    "# lambda functions \n",
    "\n",
    "data1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n",
    "print(data1['Title'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert categorical data to dummy variables for mathematical analysis. We're going to encode using the inherent sklearn and pandas tools, nothing fancy. \n",
    "\n",
    "I know that SciKit has a new library called ColumnTransformer that has replaced LabelEncoding but I haven't learned how to use it yet. \n",
    "\n",
    "https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621\n",
    "\n",
    "Let's try to use this next time:\n",
    "https://towardsdatascience.com/columntransformer-in-scikit-for-labelencoding-and-onehotencoding-in-machine-learning-c6255952731b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in data_cleaner:\n",
    "    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n",
    "    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n",
    "    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n",
    "    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n",
    "    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n",
    "\n",
    "# We now can define a y variable, the target outcome:\n",
    "Target = ['Survived']\n",
    "\n",
    "# Defining the X variables for feature selection\n",
    "data1_x = ['Sex', 'Pclass', 'Embarked', 'Title', 'SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] # pretty names for charts\n",
    "data1_x_calc = ['Sex_Code', 'Pclass', 'Embarked_Code', 'Title_Code', 'SibSp', 'Parch', 'Age', 'Fare'] # These are the actual coded columns we're gonna use\n",
    "\n",
    "data1_xy = Target + data1_x # combining them \n",
    "\n",
    "# Define the x variables for the origiunal data with bin features to remove any continuous variables\n",
    "data1_x_bin = ['Sex_Code', 'Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\n",
    "data_xy_bin = Target + data1_x_bin  # so what we've done is made two versions of the test data, one continuous and one binned\n",
    "\n",
    "# We need to turn the categorical variables into dummy variables, putting them in their own columns with binary 1/0 data inside them.\n",
    "data1_dummy = pd.get_dummies(data1[data1_x]) # ran this on that list of column names from above\n",
    "data1_x_dummy = data1_dummy.columns.tolist()\n",
    "data1_xy_dummy = Target + data1_x_dummy # add the target variable, naturally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Title_Master</th>\n",
       "      <th>Title_Misc</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  SibSp  Parch   Age     Fare  FamilySize  IsAlone  Sex_female  \\\n",
       "0       3      1      0  22.0   7.2500           2        0           0   \n",
       "1       1      1      0  38.0  71.2833           2        0           1   \n",
       "2       3      0      0  26.0   7.9250           1        1           1   \n",
       "3       1      1      0  35.0  53.1000           2        0           1   \n",
       "4       3      0      0  35.0   8.0500           1        1           0   \n",
       "\n",
       "   Sex_male  Embarked_C  Embarked_Q  Embarked_S  Title_Master  Title_Misc  \\\n",
       "0         1           0           0           1             0           0   \n",
       "1         0           1           0           0             0           0   \n",
       "2         0           0           0           1             0           0   \n",
       "3         0           0           0           1             0           0   \n",
       "4         1           0           0           1             0           0   \n",
       "\n",
       "   Title_Miss  Title_Mr  Title_Mrs  \n",
       "0           0         1          0  \n",
       "1           0         0          1  \n",
       "2           1         0          0  \n",
       "3           0         0          1  \n",
       "4           0         1          0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1_dummy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've put together all our data, we need to split some test and train data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first split using randomstate\n",
    "train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0) # using the preset variables from above to keep this function clean. \n",
    "train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target], random_state = 0) # doing the same, but with bins\n",
    "\n",
    "train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is clean, we're going to explore the data and see if we can find some basic correlations to inform what we do next. I have some ideas, but for the sake of science let's examine everything equally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival Correlation by: Sex\n",
      "      Sex  Survived\n",
      "0  female  0.742038\n",
      "1    male  0.188908\n",
      "Survival Correlation by: Pclass\n",
      "   Pclass  Survived\n",
      "0       1  0.629630\n",
      "1       2  0.472826\n",
      "2       3  0.242363\n",
      "Survival Correlation by: Embarked\n",
      "  Embarked  Survived\n",
      "0        C  0.553571\n",
      "1        Q  0.389610\n",
      "2        S  0.339009\n",
      "Survival Correlation by: Title\n",
      "    Title  Survived\n",
      "0  Master  0.575000\n",
      "1    Misc  0.444444\n",
      "2    Miss  0.697802\n",
      "3      Mr  0.156673\n",
      "4     Mrs  0.792000\n",
      "Survival Correlation by: SibSp\n",
      "   SibSp  Survived\n",
      "0      0  0.345395\n",
      "1      1  0.535885\n",
      "2      2  0.464286\n",
      "3      3  0.250000\n",
      "4      4  0.166667\n",
      "5      5  0.000000\n",
      "6      8  0.000000\n",
      "Survival Correlation by: Parch\n",
      "   Parch  Survived\n",
      "0      0  0.343658\n",
      "1      1  0.550847\n",
      "2      2  0.500000\n",
      "3      3  0.600000\n",
      "4      4  0.000000\n",
      "5      5  0.200000\n",
      "6      6  0.000000\n",
      "Survival Correlation by: FamilySize\n",
      "   FamilySize  Survived\n",
      "0           1  0.303538\n",
      "1           2  0.552795\n",
      "2           3  0.578431\n",
      "3           4  0.724138\n",
      "4           5  0.200000\n",
      "5           6  0.136364\n",
      "6           7  0.333333\n",
      "7           8  0.000000\n",
      "8          11  0.000000\n",
      "Survival Correlation by: IsAlone\n",
      "   IsAlone  Survived\n",
      "0        0  0.505650\n",
      "1        1  0.303538\n",
      "Survived    0    1\n",
      "Title             \n",
      "Master     17   23\n",
      "Misc       15   12\n",
      "Miss       55  127\n",
      "Mr        436   81\n",
      "Mrs        26   99\n"
     ]
    }
   ],
   "source": [
    "for x in data1_x:\n",
    "     if data1[x].dtype != 'float64' : \n",
    "            print('Survival Correlation by:', x)\n",
    "            print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n",
    "\n",
    "print(pd.crosstab(data1['Title'], data1[Target[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
